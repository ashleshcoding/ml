1st program

import pandas as pd 
import numpy as np 
import seaborn as sns 
import matplotlib.pyplot as plt 
from sklearn.datasets import fetch_california_housing# Step 1: Load the California Housing dataset 
data = fetch_california_housing(as_frame=True) 
housing_df = data.frame# Step 2: Create histograms for numerical features 
numerical_features = housing_df.select_dtypes(include=[np.number]).columns# Plot histograms 
plt.figure(figsize=(15, 10)) 
for i, feature in enumerate(numerical_features): 
plt.subplot(3, 3, i + 1) 
sns.histplot(housing_df[feature], kde=True, bins=30, color='blue') 
plt.title(f'Distribution of {feature}') 
plt.tight_layout() 
plt.show()# Step 3: Generate box plots for numerical features 
plt.figure(figsize=(15, 10)) 
for i, feature in enumerate(numerical_features): 
plt.subplot(3, 3, i + 1) 
sns.boxplot(x=housing_df[feature], color='orange') 
plt.title(f'Box Plot of {feature}') 
plt.tight_layout() 
plt.show()# Step 4: Identify outliers using the IQR method 
print("Outliers Detection:") 
outliers_summary = {} 
for feature in numerical_features: 
Q1 = housing_df[feature].quantile(0.25) 
Q3 = housing_df[feature].quantile(0.75) 
IQR = Q3 - Q1 
lower_bound = Q1 - 1.5 * IQR 
upper_bound = Q3 + 1.5 * IQR 
outliers = housing_df[(housing_df[feature] < lower_bound) | (housing_df[feature] > upper_bound)] 
outliers_summary[feature] = len(outliers) 
print(f"{feature}: {len(outliers)} outliers")# Optional: Print a summary of the dataset 
print("\nDataset Summary:") 
print(housing_df.describe()) 




2nd program 


import pandas as pd 
import seaborn as sns 
import matplotlib.pyplot as plt 
from sklearn.datasets import fetch_california_housing# Step 1: Load the California Housing 
Dataset 
california_data = fetch_california_housing(as_frame=True) 
data = california_data.frame# Step 2: Compute the correlation matrix 
correlation_matrix = data.corr()# Step 3: Visualize the correlation matrix using a heatmap 
plt.figure(figsize=(10, 8)) 
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5) 
plt.title('Correlation Matrix of California Housing Features') 
plt.show()# Step 4: Create a pair plot to visualize pairwise relationships 
sns.pairplot(data, diag_kind='kde', plot_kws={'alpha': 0.5}) 
plt.suptitle('Pair Plot of California Housing Features', y=1.02) 
plt.show()



3rd program

import numpy as np 
import pandas as pd 
from sklearn.datasets import load_iris 
from sklearn.decomposition import PCA 
import matplotlib.pyplot as plt# Load the Iris dataset 
iris = load_iris() 
data = iris.data 
labels = iris.target 
label_names = iris.target_names# Convert to a DataFrame for better visualization 
iris_df = pd.DataFrame(data, columns=iris.feature_names)# Perform PCA to reduce 
dimensionality to 2 
pca = PCA(n_components=2) 
data_reduced = pca.fit_transform(data)# Create a DataFrame for the reduced data 
reduced_df = pd.DataFrame(data_reduced, columns=['Principal Component 1', 'Principal 
Component 2']) 
reduced_df['Label'] = labels# Plot the reduced data 
plt.figure(figsize=(8, 6)) 
colors = ['r', 'g', 'b'] 
for i, label in enumerate(np.unique(labels)): 
plt.scatter( 
reduced_df[reduced_df['Label'] == label]['Principal Component 1'], 
reduced_df[reduced_df['Label'] == label]['Principal Component 2'], 
label=label_names[label], 
color=colors[i] 
)plt.title('PCA on Iris Dataset') 
plt.xlabel('Principal Component 1') 
plt.ylabel('Principal Component 2') 
plt.legend() 
plt.grid() 
plt.show() 


4th program

import pandas as pd 
 
 
def find_s_algorithm(file_path): 
    data = pd.read_csv(file_path) 
 
    print("Training data:") 
    print(data) 
 
    attributes = data.columns[:-1] 
    class_label = data.columns[-1] 
 
    hypothesis = ['?' for _ in attributes] 
 
    for index, row in data.iterrows(): 
        if row[class_label] == 'Yes': 
            for i, value in enumerate(row[attributes]): 
                if hypothesis[i] == '?' or hypothesis[i] == value: 
                    hypothesis[i] = value 
                else: 
                    hypothesis[i] = '?' 
 
    return hypothesis 
 
 
file_path = 'training_data.csv' 
hypothesis = find_s_algorithm(file_path) 
print("\nThe final hypothesis is:", hypothesis) 



5th program

import numpy as np 
import matplotlib.pyplot as plt 
from collections import Counterdata = np.random.rand(100)labels = ["Class1" if x <= 0.5 
else "Class2" for x in data[:50]]def euclidean_distance(x1, x2): 
return abs(x1 - x2)def knn_classifier(train_data, train_labels, test_point, k): 
distances = [(euclidean_distance(test_point, train_data[i]), train_labels[i]) for i in 
range(len(train_data))]distances.sort(key=lambda x: x[0]) 
k_nearest_neighbors = distances[:k]k_nearest_labels = [label for _, label in 
k_nearest_neighbors]return Counter(k_nearest_labels).most_common(1)[0][0]train_data = 
data[:50] 
train_labels = labelstest_data = data[50:]k_values = [1, 2, 3, 4, 5, 20, 30]print("--- k-Nearest 
Neighbors Classification ---") 
print("Training dataset: First 50 points labeled based on the rule (x <= 0.5 -> Class1, x > 0.5 
> Class2)") 
print("Testing dataset: Remaining 50 points to be classified\n")results = {}for k in k_values: 
print(f"Results for k = {k}:") 
classified_labels = [knn_classifier(train_data, train_labels, test_point, k) for test_point in 
test_data] 
results[k] = classified_labelsfor i, label in enumerate(classified_labels, start=51): 
print(f"Point x{i} (value: {test_data[i - 51]:.4f}) is classified as {label}") 
print("\n")print("Classification complete.\n")for k in k_values: 
classified_labels = results[k] 
class1_points = [test_data[i] for i in range(len(test_data)) if classified_labels[i] == "Class1"] 
class2_points = [test_data[i] for i in range(len(test_data)) if classified_labels[i] == 
"Class2"]plt.figure(figsize=(10, 6)) 
plt.scatter(train_data, [0] * len(train_data), c=["blue" if label == "Class1" else "red" for label 
in train_labels], 
label="Training Data", marker="o") 
plt.scatter(class1_points, [1] * len(class1_points), c="blue", label="Class1 (Test)", 
marker="x") 
plt.scatter(class2_points, [1] * len(class2_points), c="red", label="Class2 (Test)", 
marker="x")plt.title(f"k-NN Classification Results for k = {k}") 
plt.xlabel("Data Points") 
plt.ylabel("Classification Level") 
plt.legend() 
plt.grid(True) 
plt.show() 